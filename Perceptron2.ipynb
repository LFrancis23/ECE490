{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5026072c",
      "metadata": {
        "id": "5026072c"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7877e8cc",
      "metadata": {
        "id": "7877e8cc"
      },
      "outputs": [],
      "source": [
        "NAME = \"\"\n",
        "COLLABORATORS = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49824e9",
      "metadata": {
        "id": "b49824e9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620e22e4",
      "metadata": {
        "id": "620e22e4"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97546b47",
      "metadata": {
        "id": "97546b47"
      },
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Perceptron_example.svg/500px-Perceptron_example.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2abb86b",
      "metadata": {
        "id": "b2abb86b"
      },
      "source": [
        "## Positive definite matrices\n",
        "\n",
        "$\\newcommand{\\bfx}{\\mathbf{x}}$\n",
        "A matrix $A$ is said to be\n",
        "\n",
        "1. Positive definite, denoted by $A \\succ 0$, if $\\bfx^\\top A \\bfx > 0$ for all $\\bfx \\ne 0$.\n",
        "2. Positive semi-definite, denoted by $A \\succeq 0$, if $\\bfx^\\top A \\bfx \\ge 0$ for all $\\bfx \\ne 0$.\n",
        "3. Negative definite, denoted by $A \\prec 0$, if $\\bfx^\\top A \\bfx < 0$ for all $\\bfx \\ne 0$.\n",
        "4. Negative semi-definite, denoted by $A \\preceq 0$, if $\\bfx^\\top A \\bfx \\le 0$ for all $\\bfx \\ne 0$.\n",
        "5. Indefinite, in all other cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d863f9",
      "metadata": {
        "id": "79d863f9"
      },
      "source": [
        "## Hessian\n",
        "\n",
        "$\\newcommand{\\calH}{\\mathcal{H}}$\n",
        "$\\newcommand{\\p}{\\partial}$\n",
        "$\\newcommand{\\ppfxx}[2]{\\frac{\\p^2 f}{\\p x_{#1}\\p x_{#2}}}$\n",
        "$\\newcommand{\\ppfx}[1]{\\frac{\\p^2 f}{\\p x_{#1}^2}}$\n",
        "$$ \\calH_{\\bfx} f(\\bfx) = \\nabla^2 f(\\bfx) \n",
        "= \\begin{bmatrix}\n",
        "\\ppfx{1} & \\dots & \\ppfxx{1}{n}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\ppfxx{n}{1} & \\dots & \\ppfx{n}\n",
        "\\end{bmatrix} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0ad2ac4",
      "metadata": {
        "id": "d0ad2ac4"
      },
      "source": [
        "## Curvatures\n",
        "\n",
        "From left to right: Positive definite $\\calH f \\succ 0$, Negative definite $\\calH f \\prec 0$, Indefinite.\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42248dd",
      "metadata": {
        "id": "b42248dd"
      },
      "source": [
        "## Level Set or Contours\n",
        "\n",
        "A level-set of a function $f(\\bfx)$ at $c$ is the curve (in 2D)  or surface (in 3D or more) where the function $f(\\bfx) = c$.\n",
        "\n",
        "$$ S(f, c) = \\{\\bfx : f(\\bfx) = c\\} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55e64f8",
      "metadata": {
        "id": "a55e64f8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def plot_contour(func):\n",
        "    x, y = np.mgrid[-3:3:21j,\n",
        "                    -3:3:21j]\n",
        "    bfx = np.array([x, y])\n",
        "    f = func(x,y)\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    ctr = ax.contour(x, y, f, 20, cmap='Blues_r')\n",
        "    ax.clabel(ctr, ctr.levels, inline=True, fontsize=6, fmt='f(x)=%.01f')\n",
        "    ax.plot([0], [0], 'ro') \n",
        "    ax.text(0, 0, '$x^*$', color='r')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b861a68",
      "metadata": {
        "id": "2b861a68"
      },
      "outputs": [],
      "source": [
        "def f(x, y): return  x**2 + y**2\n",
        "plot_contour(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174bf2c4",
      "metadata": {
        "id": "174bf2c4"
      },
      "source": [
        "## Gradient visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea9f858",
      "metadata": {
        "id": "fea9f858"
      },
      "outputs": [],
      "source": [
        "def plot_gradients(func, gradfunc):\n",
        "    x, y = np.mgrid[-3:3:21j,\n",
        "                    -3:3:21j]\n",
        "    bfx = np.array([x, y])\n",
        "    f = func(x,y)\n",
        "    [dfdx, dfdy] = gradfunc(x,y)\n",
        "    fig, ax = plt.subplots()\n",
        "    ctr = ax.contour(x, y, f, 20, cmap='Blues_r')\n",
        "    ax.quiver(x, y, dfdx, dfdy)\n",
        "    ax.plot([0], [0], 'ro') \n",
        "    ax.text(0, 0, '$x^*$', color='r')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4af3c00",
      "metadata": {
        "id": "b4af3c00"
      },
      "outputs": [],
      "source": [
        "def f(x, y): return  x**2 + y**2\n",
        "def gradf(x, y): return 2*x, 2*y\n",
        "plot_gradients(f, gradf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0aba30",
      "metadata": {
        "id": "7d0aba30"
      },
      "source": [
        "## Directional Derivative\n",
        "\n",
        "$\\newcommand{\\bfd}{\\mathbf{d}}$\n",
        "The directional derivatiive is change of the function $f(\\bfx)$ in the direction $\\bfd$\n",
        "\n",
        "$$ D_{\\bfd} f(\\bfx) = \\lim_{h \\to 0} \\frac{f(\\bfx + h \\bfd) - f(\\bfx)}{h}$$\n",
        "\n",
        "The partial derivatives can be interpreted as the derivatives in the direction of axis:\n",
        "$$ \\frac{\\p f(\\bfx)}{\\p x} = \\lim_{h \\to 0} \\frac{f(\\bfx + h \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix} ) - f(\\bfx)}{h}$$\n",
        "$$ \\frac{\\p f(\\bfx)}{\\p y} = \\lim_{h \\to 0} \\frac{f(\\bfx + h \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix} ) - f(\\bfx)}{h}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "116114be",
      "metadata": {
        "id": "116114be"
      },
      "source": [
        "## Gradient is perpendicular to contours or level-sets\n",
        "\n",
        "Let the implicit form of the level set be:\n",
        "\n",
        "$$ S(f, c) = \\{\\bfx : f(\\bfx) = c\\} $$\n",
        "\n",
        "\n",
        "Let it's parameteric form the curve be:\n",
        "\n",
        "$\\newcommand{\\bfg}{\\mathbf{g}}$\n",
        "$\\newcommand{\\bft}{\\mathbf{t}}$\n",
        "$\\newcommand{\\bbR}{\\mathbf{R}}$\n",
        "$$ S(f, c) = \\{\\bfg(c, \\bft): \\bft \\in \\bbR^{n-1}\\} $$\n",
        "\n",
        "For a constant $c$, we have $f(\\bfg(c, \\bft)) = c$. Differentiating it with respect to $\\bft$, we get by chain rule.\n",
        "\n",
        "$\\newcommand{\\calJ}{\\mathcal{J}}$\n",
        "$$ \\nabla_\\bfx f(\\bfx)^\\top \\calJ_\\bft \\bfg(c, \\bft) = \\mathbf{0}^\\top$$,\n",
        "where $\\calJ_\\bft \\bfg(c, \\bft)$ is the Jacobian of $\\bfg(c, \\bft)$.\n",
        "\n",
        "Dot product is zero when the two vectors are perpendicular. This means that the gradient is always perpendicular to the tangents to the surface $\\calJ_\\bft \\bfg(c, \\bft)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beaa59bc",
      "metadata": {
        "id": "beaa59bc"
      },
      "source": [
        "### Quadratic function example:\n",
        "\n",
        "A quadratic function $f(\\bfx) = x_1^2 + x_2^2$ has level sets as circles:\n",
        "$$ S(f, c) = \\{\\bfx : x_1^2 + x_2^2 = c\\} $$\n",
        "\n",
        "It has the parameteric form as\n",
        "$$ S(f, c) = \\{\\bfg(c, \\theta) = \\begin{bmatrix}\\sqrt{c} \\cos(\\theta) \\\\ \\sqrt{c} \\sin(\\theta)\\end{bmatrix} : \\theta \\in [0, 2\\pi)\\} $$\n",
        "\n",
        "The gradient is:\n",
        "$$ \\nabla_\\bfx f(\\bfx)^\\top = 2\\bfx^\\top \n",
        "= 2 \\begin{bmatrix} \\sqrt{c} \\cos(\\theta) & \\sqrt{c} \\sin(\\theta)\\end{bmatrix}$$\n",
        "\n",
        "and \n",
        "\n",
        "The derivative of curve with respect to $\\theta$ the tangent to the curve:\n",
        "$$\\calJ_\\theta \\bfg(c, \\theta) \n",
        "= \\begin{bmatrix} \\frac{\\p }{\\p \\theta} \\sqrt{c} \\cos(\\theta) \\\\ \\frac{\\p }{\\p \\theta} \\sqrt{c} \\sin(\\theta)\\end{bmatrix} \n",
        "= \\begin{bmatrix}-\\sqrt{c} \\sin(\\theta) \\\\ \\sqrt{c} \\cos(\\theta)\\end{bmatrix}$$\n",
        "\n",
        "$$ \\nabla_\\bfx f(\\bfx)^\\top\\calJ_\\theta \\bfg(c, \\theta) \n",
        "= 2 \\begin{bmatrix} \\sqrt{c} \\cos(\\theta) & \\sqrt{c} \\sin(\\theta)\\end{bmatrix}\n",
        "\\begin{bmatrix}-\\sqrt{c} \\sin(\\theta) \\\\ \\sqrt{c} \\cos(\\theta)\\end{bmatrix} = 0$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d2589f",
      "metadata": {
        "id": "c0d2589f"
      },
      "source": [
        "## Minimizing general functions\n",
        "\n",
        "We cannot minimize general functions by solving\n",
        "\n",
        "$$ \\frac{\\p f(\\bfx)}{\\p \\bfx} = \\mathbf{0}^\\top$$ \n",
        "\n",
        "because the equation might not have a formula for it.\n",
        "\n",
        "Instead we use iterative methods like gradient descent minimize general function $f(\\bfx)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2efff3a",
      "metadata": {
        "id": "f2efff3a"
      },
      "source": [
        "## Gradient descent\n",
        "\n",
        "1. Start from a random point $\\bfx_0$, $\\bfx_t \\leftarrow \\bfx_0$.\n",
        "2. Move in the direction opposite to $\\nabla_\\bfx f(\\bfx)$. If we were at $\\bfx_t$, then the next point is at $\\bfx_{t+1} = \\bfx_t - \\alpha_t \\nabla_\\bfx f(\\bfx)$, where $\\alpha_t > 0$ is a positive scalar, called the step size or the learning rate.\n",
        "3. Stop when the gradient is almost zero $\\|\\nabla_\\bfx f(\\bfx)\\| < 10^{-4}$.\n",
        "\n",
        "This corresponds to the following algorithm:\n",
        "\n",
        "$\\bfx_t = \\bfx_0$<br/>\n",
        "while ($\\|\\nabla_\\bfx f(\\bfx)\\| > 10^{-4}$) {\n",
        "    $\\bfx_t \\leftarrow \\bfx_t - \\alpha_t \\nabla_\\bfx f(\\bfx)$\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f498e0f3",
      "metadata": {
        "id": "f498e0f3"
      },
      "source": [
        "## A bit more about the step size/learning rate\n",
        "\n",
        "Recall the Taylor series expansion of a function $f(x)$ around $x_0$\n",
        "\n",
        "$$f(x) = f(x_0) + \\frac{d f(x)}{d x} (x - x_0) + \\frac{1}{2!} \\frac{d^2 f(x)}{d x^2} (x - x_0)^2 + \\dots + \\frac{1}{n!} \\frac{d^n f(x)}{d x^n} (x - x_0)^n + \\dots $$\n",
        "\n",
        "Vectorized Taylor series expansion is\n",
        "\n",
        "$$f(\\bfx) = f(\\bfx_0) + \\nabla_\\bfx^\\top f(\\bfx) (\\bfx - \\bfx_0) + \\frac{1}{2!}  (\\bfx - \\bfx_0)^\\top \\calH f(\\bfx) (\\bfx - \\bfx_0)^\\top + \\dots \\infty$$\n",
        "\n",
        "While optimizing around the point $\\bfx_t$, we can use the Taylor series expansion to find a local quadratic approximation to find the next best minima:\n",
        "\n",
        "$$\\hat{f}(\\bfx_{t+1}) = f(\\bfx_t) + \\nabla_\\bfx^\\top f(\\bfx) (\\bfx_{t+1} - \\bfx_t) + \\frac{1}{2!}  (\\bfx_{t+1} - \\bfx_t)^\\top \\calH f(\\bfx) (\\bfx_{t+1} - \\bfx_t)^\\top $$\n",
        "\n",
        "At the optimal point of the quadratic approximation the derivative is zero:\n",
        "$$\\frac{\\p \\hat{f}(\\bfx_{t+1})}{\\p \\bfx_{t+1}} = \\mathbf{0}^\\top$$\n",
        "$$\\nabla_\\bfx^\\top f(\\bfx) + (\\bfx_{t+1} - \\bfx_t)^\\top \\calH  f(\\bfx) = \\mathbf{0}^\\top$$\n",
        "\n",
        "Taking transpose and rearranging the terms we get:\n",
        "$$ \\bfx_{t+1} - \\bfx_t = -[\\calH  f(\\bfx)]^{-1}\\nabla_\\bfx f(\\bfx) $$\n",
        "$$ \\bfx_{t+1} = \\bfx_t -[\\calH  f(\\bfx)]^{-1}\\nabla_\\bfx f(\\bfx) $$\n",
        "\n",
        "This is the update rule for the Newton's method for optimization. Note that the step size here is inversely proportional the second derivative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f5fa73",
      "metadata": {
        "id": "01f5fa73"
      },
      "source": [
        "### Taylor series approximation\n",
        "\n",
        "Approximate the following function to a quadratic function near the point $\\bfx_0 = [-2, 3]$\n",
        "\n",
        "$f (x) = 0.06\\exp( 2x_1 +x_2) + 0.05\\exp(x_1−2 x_2) + \\exp(−x_1) $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0c6cdc",
      "metadata": {
        "id": "1a0c6cdc"
      },
      "outputs": [],
      "source": [
        "# Define the function\n",
        "def f(x): \n",
        "    return (0.06 * np.exp(x @ [2, 1]) \n",
        "            + 0.05* np.exp(x @ [1, -2]) \n",
        "            + np.exp(x @ [-1, 0]))\n",
        "\n",
        "# Compute its derivative, the gradient function\n",
        "def grad_f(x):    \n",
        "    coeff1 = np.array([2, 1])\n",
        "    coeff2 = np.array([1, -2])\n",
        "    coeff3 = np.array([-1, 0])\n",
        "    # Slicing using np.newaxis or None, increases the dimension by 1.\n",
        "    # https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis\n",
        "    return (0.06 * np.exp(x @ coeff1)[..., None] * coeff1\n",
        "            + 0.05 * np.exp(x @ coeff2)[..., None] * coeff2\n",
        "            + np.exp(x @ coeff3)[..., None] * coeff3)\n",
        "\n",
        "def numerical_jacobian(f, x, h=1e-10):\n",
        "    n = x.shape[-1]\n",
        "    eye = np.eye(n)\n",
        "    x_plus_dx = x + h * eye # n x n\n",
        "    num_jac = (f(x_plus_dx) - f(x)) / h # limit definition of the formula # n x m\n",
        "    if num_jac.ndim >= 2:\n",
        "        num_jac = num_jac.swapaxes(-1, -2) # m x n\n",
        "    return num_jac\n",
        "    \n",
        "# Compare our grad_f with numerical gradient\n",
        "def check_numerical_jacobian(f, jac_f,  nD=2, **kwargs):\n",
        "    x = np.random.rand(nD)\n",
        "    num_jac = numerical_jacobian(f, x, **kwargs)\n",
        "    return np.allclose(num_jac, jac_f(x), atol=1e-06, rtol=1e-4) # m x n\n",
        "\n",
        "## Throw error if grad_f is wrong\n",
        "assert check_numerical_jacobian(f, grad_f)\n",
        "\n",
        "def hessian_f(x):\n",
        "    coeff1 = np.array([2, 1])\n",
        "    coeff2 = np.array([1, -2])\n",
        "    coeff3 = np.array([-1, 0])\n",
        "    return (0.06 * np.exp(x @ coeff1)[..., None, None] * np.outer(coeff1, coeff1)\n",
        "            + 0.05 * np.exp(x @ coeff2)[..., None, None] * np.outer(coeff2, coeff2)\n",
        "            +  np.exp(x @ coeff3)[..., None, None] * np.outer(coeff3, coeff3))\n",
        "\n",
        "## Throw error if hessian_f is wrong\n",
        "assert check_numerical_jacobian(grad_f, hessian_f)\n",
        "\n",
        "\n",
        "def taylor_series_quad_approx(x0, func, grad_func, hessian_func):\n",
        "    def quad_func(x):\n",
        "        x_min_x0 = (x-x0)[..., None] # make column vectors\n",
        "        x_min_x0_T = (x-x0)[..., None, :] # make row vectors\n",
        "        grad_f_x0_T = grad_func(x0)[..., None, :] # make row vectors\n",
        "        return (func(x0) \n",
        "                + grad_f_x0_T @ x_min_x0\n",
        "                + x_min_x0_T @ hessian_func(x0) @ x_min_x0).squeeze(axis=(-1,-2))\n",
        "            \n",
        "    return quad_func\n",
        "\n",
        "def plot_contours(func, ax=None, cmap='Blues_r', levels=20, \n",
        "                  xrange=slice(-3,3,21j),\n",
        "                  yrange=slice(-3,3,21j)):\n",
        "    x, y = np.mgrid[xrange,\n",
        "                    yrange]\n",
        "    bfx = np.concatenate([x[..., None],\n",
        "                          y[..., None]], axis=-1)\n",
        "    f = func(bfx)\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    ctr = ax.contour(x, y, np.log(f), levels, cmap=cmap)\n",
        "    ax.clabel(ctr, ctr.levels, inline=True, fontsize=6)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    return ax\n",
        "    \n",
        "# Fit a quadratic curve around this curve:\n",
        "ax = plot_contours(f, \n",
        "                   levels=[0.01, 0.5, 0.8, 1.0, 1.2, 1.6, 1.8, 2.0])\n",
        "x0 = np.array([0, 2])\n",
        "ax.plot([x0[0]], [x0[1]], 'ro')\n",
        "ax.text(x0[0], x0[1], '$x_0$')\n",
        "quad_func = taylor_series_quad_approx(x0, f, grad_f, hessian_f)\n",
        "# x, y = np.mgrid[-3:3:21j,\n",
        "#                     -3:3:21j]\n",
        "# bfx = np.concatenate([x[..., None],\n",
        "#                       y[..., None]], axis=-1)\n",
        "# print(bfx.shape)\n",
        "# print(f(bfx).shape)\n",
        "# print(grad_f(bfx).shape)\n",
        "# print(quad_func(bfx).shape)\n",
        "plot_contours(quad_func, ax=ax, cmap='Reds_r', \n",
        "              levels=[0.1, 0.2, 0.5, 0.8, 1.0, 1.5],\n",
        "              xrange=slice(-1,2,21j),\n",
        "              yrange=slice(-2,3,21j))\n",
        "ax.set_title(\"Taylor series quadratic fit in red to the blue curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a8b5688",
      "metadata": {
        "id": "7a8b5688"
      },
      "source": [
        "## Minimization by gradient descent\n",
        "\n",
        "Find the minimizer of $f (\\bfx) = 0.06\\exp( 2x_1 +x_2) + 0.05\\exp(x_1−2 x_2) + \\exp(−x_1) $."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feced659",
      "metadata": {
        "id": "feced659"
      },
      "source": [
        "In vector form we can write it as:\n",
        "\n",
        "$$f (\\bfx) = 0.06\\exp( [2, 1] \\bfx ) + 0.05\\exp([1, -2] \\bfx) + \\exp([-1, 0] \\bfx) $$\n",
        "$$ \\nabla_\\bfx^\\top f(\\bfx) = 0.06\\exp( [2, 1] \\bfx )[2, 1] + 0.05\\exp([1, -2] \\bfx)[1, -2] + \\exp([-1, 0] \\bfx) [-1, 0] $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f819942",
      "metadata": {
        "id": "3f819942"
      },
      "outputs": [],
      "source": [
        "# Define the function\n",
        "def f(x):\n",
        "    return (0.06 * np.exp(x @ [2, 1]) \n",
        "            + 0.05* np.exp(x @ [1, -2]) \n",
        "            + np.exp(x @ [-1, 0]))\n",
        "\n",
        "# Compute its derivative, the gradient function\n",
        "def grad_f(x):    \n",
        "    coeff1 = np.array([2, 1])\n",
        "    coeff2 = np.array([1, -2])\n",
        "    coeff3 = np.array([-1, 0])\n",
        "    # Slicing using None, increases the dimension by 1.\n",
        "    # \n",
        "    return (0.06 * np.exp(x @ coeff1)[..., None] * coeff1\n",
        "            + 0.05 * np.exp(x @ coeff2)[..., None] * coeff2\n",
        "            + np.exp(x @ coeff3)[..., None] * coeff3)\n",
        "\n",
        "\n",
        "assert check_numerical_jacobian(f, grad_f)\n",
        "f(np.zeros((2,))), grad_f(np.zeros((2,))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d86c25",
      "metadata": {
        "id": "25d86c25"
      },
      "outputs": [],
      "source": [
        "def plot_gradients(func, gradfunc):\n",
        "    x, y = np.mgrid[-3:3:21j,\n",
        "                    -3:3:21j]\n",
        "    bfx = np.concatenate((x[..., None], y[..., None]), axis=-1)\n",
        "    f = func(bfx)\n",
        "    dfdx = gradfunc(bfx)\n",
        "    fig, ax = plt.subplots()\n",
        "    ctr = ax.contour(x, y, np.log(f), 20, cmap='Blues_r')\n",
        "    ax.quiver(bfx[..., 0], bfx[..., 1], dfdx[..., 0], dfdx[..., 1])\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.axis('equal')\n",
        "    plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f75d5a",
      "metadata": {
        "id": "25f75d5a"
      },
      "outputs": [],
      "source": [
        "plot_gradients(f, grad_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f2aafd",
      "metadata": {
        "id": "59f2aafd"
      },
      "outputs": [],
      "source": [
        "# Implement the gradient descent algorithm\n",
        "def minimize(x0, f, grad_func, alpha_t=0.2, maxiter=100):\n",
        "    t = 0\n",
        "    xt = x0\n",
        "    grad_f_t = grad_func(xt)\n",
        "    list_of_xts, list_of_fs = [xt], [f(xt)] # for logging\n",
        "    while np.linalg.norm(grad_f_t) > 1e-4: # <-- Check for convergence\n",
        "        xt = xt - alpha_t * grad_f_t # <-- Main update step\n",
        "        grad_f_t = grad_func(xt) # Compute the next gradient\n",
        "        \n",
        "        if t >= maxiter:  # Failsafe, if the algorithm does not converge\n",
        "            break\n",
        "        else:\n",
        "            t += 1\n",
        "        list_of_xts.append(xt) # for logging\n",
        "        list_of_fs.append(f(xt)) # for logging\n",
        "    \n",
        "    return xt, list_of_xts, list_of_fs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0ede49",
      "metadata": {
        "id": "8e0ede49"
      },
      "outputs": [],
      "source": [
        "x0 = np.array([-2, 2])\n",
        "#x0 = np.random.rand(2,2) * 4 - 2\n",
        "OPTIMAL_X, list_of_xts, list_of_fs = minimize(x0,\n",
        "                                              f, grad_f, \n",
        "                                              alpha_t=0.2,\n",
        "                                              maxiter=1000)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(list_of_fs)\n",
        "ax.set_xlabel('t')\n",
        "ax.set_ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21506739",
      "metadata": {
        "id": "21506739"
      },
      "outputs": [],
      "source": [
        "from matplotlib import animation, rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "class Anim:\n",
        "    def __init__(self, fig, ax, func):\n",
        "        self.fig = fig\n",
        "        self.ax = ax\n",
        "        x, y = np.mgrid[-3:3:21j,\n",
        "                        -3:3:21j]\n",
        "        bfx = np.concatenate((x[..., None], y[..., None]), axis=-1)\n",
        "        f = func(bfx)\n",
        "        self.ctr = self.ax.contour(x, y, np.log(f), 20, cmap='Blues_r')\n",
        "        self.ax.set_xlabel('x')\n",
        "        self.ax.set_ylabel('y')\n",
        "        #self.ax.clabel(self.ctr, self.ctr.levels, inline=True, fontsize=6)\n",
        "        self.list_of_xs = []\n",
        "        self.list_of_ys = []\n",
        "        self.line2, = self.ax.plot([], [], 'r*-')\n",
        "\n",
        "        \n",
        "    def anim_init(self):\n",
        "        return (self.line2,)\n",
        "        \n",
        "    def update(self, xt):\n",
        "        self.list_of_xs.append(xt[0])\n",
        "        self.list_of_ys.append(xt[1])\n",
        "        self.line2.set_data(self.list_of_xs, self.list_of_ys)\n",
        "        return self.line2,\n",
        "    \n",
        "fig, ax = plt.subplots()        \n",
        "a = Anim(fig, ax, f)\n",
        "animation.FuncAnimation(fig, a.update, frames=list_of_xts[::5],\n",
        "                        init_func=a.anim_init, blit=True, repeat=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be45d64f",
      "metadata": {
        "id": "be45d64f"
      },
      "outputs": [],
      "source": [
        "# The learning rate can be sensitive to the starting points. \n",
        "# Observe the behavior for different starting points\n",
        "# For different starting points you might need different learning rate scheme\n",
        "\n",
        "x0 = np.random.rand(2) * 6 - 3\n",
        "OPTIMAL_X, list_of_xts, list_of_fs = minimize(x0,\n",
        "                                              f, grad_f, \n",
        "                                              alpha_t=0.2,\n",
        "                                              maxiter=200)\n",
        "fig, ax = plt.subplots()        \n",
        "a = Anim(fig, ax, f)\n",
        "animation.FuncAnimation(fig, a.update, frames=list_of_xts[::10],\n",
        "                        init_func=a.anim_init, blit=True, repeat=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eae546d",
      "metadata": {
        "id": "7eae546d"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Following the example above, fit a quadratic function to the function using Taylor series expansion near the points $\\bfx_0 = \\begin{bmatrix} -1 \\\\ 1\\end{bmatrix}$ then visualize the contour plots of the original function and the quadratic function (50 marks),\n",
        "\n",
        "$$f(\\bfx) = x_1 \\exp(-(x_1^2 + x_2^2)) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288af3c2",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "305256c6007607eb7ac224e48bb3bb17",
          "grade": false,
          "grade_id": "cell-4dccb852958c96b5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "288af3c2"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b21df85",
      "metadata": {
        "id": "2b21df85"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Following the example above, *implement your own* gradient descent algorithm that minimizes the following function (50 marks),\n",
        "\n",
        "$$f(\\bfx) = x_1 \\exp(-(x_1^2 + x_2^2)) $$\n",
        "\n",
        "Test your algorithm with the starting points of $\\bfx_0 = \\begin{bmatrix} -1 \\\\ 1\\end{bmatrix}$ and $\\bfx_0 = \\begin{bmatrix} -1 \\\\ -1\\end{bmatrix}$ and learning rate of $\\alpha_t = 0.25$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7488f3a-9d38-487f-b2ec-fee1bca25cb8",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c922d140281c6932c1b7c595cc5af53d",
          "grade": false,
          "grade_id": "cell-62edd01f5331c45a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "c7488f3a-9d38-487f-b2ec-fee1bca25cb8"
      },
      "outputs": [],
      "source": [
        "x0 = np.array([-1, 1])\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "print(OPTIMAL_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56329de5-45ab-4484-9a39-e577f66390bb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d7f7e7497530d5e64013121053591359",
          "grade": true,
          "grade_id": "cell-2802bfb4b2c6edaf",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "56329de5-45ab-4484-9a39-e577f66390bb"
      },
      "outputs": [],
      "source": [
        "assert np.allclose(OPTIMAL_X, [-7.07e-01,  1.06e-04], atol=1e-3, rtol=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a8dda3-fdd0-4b93-ad06-3b4405aa1716",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2204d2fe16cddb5d6b5f54cb73c88eaf",
          "grade": false,
          "grade_id": "cell-729ffc121c39078d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": [],
        "id": "19a8dda3-fdd0-4b93-ad06-3b4405aa1716"
      },
      "outputs": [],
      "source": [
        "x0 = np.array([-1, -1])\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "print(OPTIMAL_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5a88d1c-e869-4e66-ae6e-90cc47f488ae",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9319c3ecad782c6613461634c95cfbf1",
          "grade": true,
          "grade_id": "cell-e58b107f406f3236",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "tags": [],
        "id": "b5a88d1c-e869-4e66-ae6e-90cc47f488ae"
      },
      "outputs": [],
      "source": [
        "assert np.allclose(OPTIMAL_X, [-7.07e-01,  1.06e-04], atol=1e-3, rtol=1e-2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}